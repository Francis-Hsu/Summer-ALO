\documentclass[11pt]{article}
\usepackage{fullpage,titling}
\usepackage{mathtools,amssymb,amsthm}
\usepackage{bm}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}
\usepackage{lstautogobble}
\usepackage[T1]{fontenc}
\usepackage{newpxtext,newpxmath}
\usepackage[activate={true,nocompatibility},final,tracking=true, kerning=true, spacing=true, factor=1100, stretch=10, shrink=10]{microtype}

\newcommand{\bu}{\bm{u}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bA}{\bm{A}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bJ}{\bm{J}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bXb}{\bm{\mathcal{X}}}
\newcommand{\bBb}{\bm{\mathcal{B}}}
\newcommand{\bPhi}{\bm{\Phi}}

\newcommand{\bbr}{\mathbb{R}} 
\newcommand{\bbq}{\mathbb{Q}}
\newcommand{\bbn}{\mathbb{N}}

\newcommand{\semicol}{\nobreak\mskip2mu\mathpunct{}\nonscript\mkern-\thinmuskip{;}\mskip6muplus1mu\relax}

\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\bprox}{\mathbf{prox}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\refthm}[2]{#1~#2}

\title{Approximate Leave-One-Out with kernel SVM}
\author{Linyun He \and Wanchao Qin \and Peng Xu \and Yuze Zhou}

\begin{document}
\maketitle

\section{ALO with Representer Theorem}
Let \(\bK\) denote the positive-definite kernel matrix (hence invertible), with \(\bK_{i,j}=K(\bx_i,\bx_j)\). By the Representer Theorem, the dual problem to kernel SVM can be expressed in ``loss \(+\) penalty'' form:
\begin{equation}
\min_{\rho,\balpha}\sum_{j=1}^{n}\left(1-y_jf(x_j)\right)_++\frac{\lambda}{2}\balpha^\top\bK\balpha, \qquad f(x_j)=\bK_{\cdot,j}^\top\balpha+\rho.
\end{equation}
For simplicity we ignore the offset for now. Let \(S\) and \(V\) be the smooth set and the singularities set, respectively. For the \(j\)-th observation, \(j\in S\) and that \(f(x_j)<1\), we have \[\dot{\ell}(\bK_{\cdot,j}^\top\balpha)=-y_j,\qquad\ddot{\ell}(\bK_{\cdot,j}^\top\balpha)=0.\] Additionally, \[\nabla R(\balpha)=\lambda\bK\balpha,\qquad\nabla^2R(\balpha)=\lambda\bK.\] Substitute corresponding terms in \refthm{Thm.}{4.1}, we deduce the ALO formula for kernel SVM: \[\bK_{\cdot,i}^\top\tilde{\balpha}^{\setminus i}=\bK_{\cdot,i}^\top\hat{\balpha}+a_ig_{\ell,i},\] where \[a_i=\begin{dcases}
\frac{1}{\lambda}\bK_{\cdot,i}^\top\left[\bK^{-1}-\bK^{-1}\bK_{\cdot,V}\left(\bK_{\cdot,V}^\top\bK^{-1}\bK_{\cdot,V}\right)^{-1}\bK_{\cdot,V}^\top\bK^{-1}\right]\bK_{\cdot,i} & i\in S,\\
\left[\lambda\left(\bK_{\cdot,V}^\top\bK^{-1}\bK_{\cdot,V}\right)^{-1}_{ii}\right]^{-1} & i\in V,
\end{dcases}\] and \[g_{\ell, S}=-y_S\odot\bm{1}\left\{y_S\bK_{\cdot,S}^\top\balpha<1\right\},\qquad g_{\ell, V}=\left(\bK_{\cdot,V}\bK_{\cdot,V}^\top\right)^{-1}\bK_{\cdot,V}\left[\lambda\bK\alpha-\sum_{j:y_j\bK_{\cdot,j}^\top\balpha<1}y_j\bK_{\cdot,j}\right].\]

\section{ALO with Approximate Explicit Feature Maps}
In non-linear SVM, kernel trick is employed to avoid the explicit computation of feature maps, which sometime is impossible since the feature space can be infinite-dimensional. However, when sample size \(n\) is large, the kernel matrices become quite expensive to handle. Methods such as the Nystr\"{o}m approximation are used in order to retain the benefit of features mapping whilst retaining the speed of linear SVM. We may adopt a similar idea to help the ALO computation.

Let \(\bX\) be the data matrix and \(\bK\) be the corresponding kernel matrix. An approximation \(\hat{\bPhi}\) to the feature maps \(\bPhi(\bX)\) can be constructed as following (procedure adopted from \verb|scikit-learn|):
	\begin{enumerate}
		\item Perform SVD: \(\bK=\bm{U}\bm{S}\bm{V}^\top\);
		\item Clamp the singular values: \(\tilde{\bm{S}}=\max(\bm{S}, 10^{-12})\);
		\item Construct the approximate map as \(\hat{\bPhi}=\bK\bm{U}\tilde{\bm{S}}^{-1/2}\bm{V}^\top\approx\bK^{1/2}\).
	\end{enumerate}
To compute ALO, we then simply replace the data matrix \(\bX\) with \(\hat{\bPhi}\) in the linear SVM formula.
\end{document}