\documentclass[11pt]{article}
\usepackage{fullpage,titling}
\usepackage{mathtools,amssymb,amsthm}
\usepackage{bm}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}
\usepackage{lstautogobble}
\usepackage[T1]{fontenc}
\usepackage{newpxtext,newpxmath}
\usepackage[activate={true,nocompatibility},final,tracking=true, kerning=true, spacing=true, factor=1100, stretch=10, shrink=10]{microtype}

\newcommand{\bu}{\bm{u}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bA}{\bm{A}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bJ}{\bm{J}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}

\newcommand{\bbr}{\mathbb{R}} 
\newcommand{\bbq}{\mathbb{Q}}
\newcommand{\bbn}{\mathbb{N}}

\newcommand{\semicol}{\nobreak\mskip2mu\mathpunct{}\nonscript\mkern-\thinmuskip{;}\mskip6muplus1mu\relax}

\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\bprox}{\mathbf{prox}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\refthm}[2]{#1~#2}

\title{Notes on Approximate Leave-One-Out for Elastic Net}
\author{Linyun He \and Wanchao Qin \and Peng Xu \and Yuze Zhou}

\begin{document}
\maketitle

\section{ALO for Elastic Net, Approximation in the Primal Domain}
Recall the objective function for the elastic net problem:
	\begin{equation}
	\min_{\bbeta}\frac{1}{2}\sum_{j=1}^{n}(\bx_j^\top\bbeta-y_j)^2+\lambda\left(\alpha\|\bbeta\|_1+\frac{1-\alpha}{2}\|\bbeta\|_2^2\right).
	\end{equation}
Let \(A=\{i:\beta_i\not\in K,i=1,\dotsc,p\}\) be the active set, we have \[\dot{\ell}(\bx_j^\top\bbeta\semicol y_j)=\bx_j^\top\bbeta-y_j,\qquad\ddot{\ell}(\bx_j^\top\bbeta\semicol y_j)=1,\qquad\nabla^2 R(\hat{\bbeta}_A)=(1-\alpha)\lambda\bI_{A,A}.\] Thus, \refthm{Eqn.}{31} reduces to
	\begin{equation}
	\bH=\bX_{\cdot,A}\left[\bX_{\cdot,A}^\top\bX_{\cdot,A}+\left(1-\alpha\right)\lambda\bI_{A,A}\right]^{-1}\bX_{\cdot,A}^\top.
	\end{equation}
By augmenting \(\bX\) with an extra column of \(1\)s, adding the intercept back to the model is straightforward, as \refthm{Eqn.}{31} now becomes 
	\begin{equation}
	\bm{H}=\left[\bm{1}_n,\bm{X}_{\cdot, A}\right]\left\{\left[\bm{1}_n,\bm{X}_{\cdot, A}\right]^\top\bm{D}\left[\bm{1}_n,\bm{X}_{\cdot, A}\right]+\nabla^2R\left(\hat{\beta}_0,\hat{\bm{\beta}}_A\right)\right\}^{-1}\left[\bm{1}_n,\bm{X}_{\cdot, A}\right]^\top,
	\end{equation}
where \[\bm{D}=\diag\left[\ddot{\ell}\left(\hat{\beta}_0+\bm{x}_j^\top\hat{\bm{\beta}};y_j\right)\right]_{j\in A}=\bI_{A,A},\qquad\nabla^2R\left(\hat{\beta}_0,\hat{\bm{\beta}}_A\right)=\begin{bmatrix}
0 & 0 & \dots & 0 \\
0 & (1-\alpha)\lambda & \dots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \dots & (1-\alpha)\lambda \\
\end{bmatrix}.\] Then, the ALO can be computed as
	\begin{equation}
	\begin{bmatrix}
	1 & \bm{x}_i^\top\end{bmatrix}
	\begin{bmatrix}
	\tilde{\beta}_0^{\setminus i} \\
	\tilde{\bm{\beta}}^{\setminus i}\end{bmatrix}=(\hat{\beta}_0+\bm{x}_i^\top\hat{\bm{\beta}})+\frac{\bH_{ii}}{1-\bH_{ii}\ddot{\ell}\left(\hat{\beta}_0+\bm{x}_i^\top\hat{\bm{\beta}};y_i\right)}\dot{\ell}\left(\hat{\beta}_0+\bm{x}_i^\top\hat{\bm{\beta}};y_i\right)
	\end{equation}

\section{ALO for Elastic Net, Approximation in the Dual Domain}
The original problem for elastic net is to solve for $\hat{\bbeta}$ such that: 
\begin{equation}
\hat{\bbeta}=\argmin\limits_{\bbeta}\left(\frac{1}{2}\|\by-\bX\bbeta\|_{2}^{2} + \lambda_{1}\|\bbeta\|_{1}+\lambda_{2}\|\bbeta\|_{2}^{2}\right)
\end{equation} 
By adding the Lagrangian, we get the formulation of $L$: 
	\begin{equation}
	L = \frac{1}{2}\|\by-\bz\|_{2}^{2} + \lambda_{1}\|\bbeta\|_{1}+\lambda_{2}\|\bbeta\|_{2}^{2}+u^{\top}(\bz-\bX\bbeta).
	\end{equation}
The original problem is solving the primal of the Lagrangian such that $p^{*} = \min\limits_{\bbeta,\bz}\max\limits_{\bu}L$ and the dual formulation $d^{*} = \max\limits_{\bu}\min\limits_{\bbeta,\bz}L$, to minimize over $\bz$: \[\frac{\partial L}{\partial \bz} = \bz -\by +\bu = \bm{0}\implies \by = \bu + \bz.\] Since $\bbeta$ is penalized element-wisely, we can minimize over $\bbeta$ by minimizing over each $\bbeta_{i}$, that is, we have to minimize $\lambda_{1}|\beta_{i}| + \lambda_{2}\beta_{i}^{2} - \bu^{\top}\bX_{i}\bbeta$ for each dimension of $\bbeta$, where $\bX_{i}$ denotes the $i$th column of $\bX$, therefore: \[\min\limits_{\bbeta}\left(\lambda_{1}|\beta_{i}| + \lambda_{2}\beta_{i}^{2} - \bu^{\top}\bX_{i}\bbeta\right)=\begin{dcases}
0 & |\bu^{\top}\bX_{i}| \leq \lambda_{1},\\
-\frac{(\lambda_{1}-|\bu^{\top}\bX_{i}|)^{2}}{4\lambda_{2}} & |\bu^{\top}\bX_{i}| > \lambda_{1}.
\end{dcases}\] By taking all the above to the Lagrangian, we obtain the dual problem $d^{*}$ as: 
	\begin{equation}
	d^{*} = \min\limits_{\bu}\frac{1}{2}\|\by-\bu\|_{2}^{2} + \sum_{j: |\bX_{j}^{\top}\bu| > \lambda_{1}}\frac{(\lambda_{1}-|\bu^{\top}\bX_{i}|)^{2}}{4\lambda_{2}}.
	\end{equation}
The minimizer $\hat{\bu}$ could also be obtained from the dual problem through a proximal approach: \[\hat{\bu} = \bprox_{R}(y),\qquad\quad R(\bu) = \sum_{j:|\bX_{j}^{\top}\bu| > \lambda_{1}}\frac{(\lambda_{1}-|\bu^{\top}\bX_{i}|)^{2}}{4\lambda_{2}}.\]

By replacing the full data problem $\by$ with $\by_{\alpha} = \by + (y_{i}^{\setminus i}-y_{i})e_{i}$, where $y_{i}^{\setminus i}$ is the true LOO estimator and $e_{i}$ is the $i$-th standard vector, and let $\bu^{\setminus i} = \bprox_{R}(\by_{\alpha})$, we have:
\begin{align*}
0 &= e_{i}^{\top}\bu^{\setminus i}\\
& = e_{i}^{\top}\bprox_{R}(\by_{\alpha})\\
& \approx e_{i}^{\top}[\bprox_{R}(\by)+\bJ_{R}(\by)(\by_{\alpha}-\by)]\\
& \approx \hat{u}_{i} + \bJ_{ii}(y_{i}^{\setminus i}-y_{i}).
\end{align*} Here $\bJ_{R}(\by)$ denotes the Jacobian matrix of the proximal operator at $\by$, thus the ALO estimator $\tilde{y}_{i}$ is obtained as 
	\begin{equation}
	\tilde{y}_{i} = y_{i} - \frac{\hat{u}_{i}}{\bJ_{ii}}.
	\end{equation}
The Jacobian could locally be obtained as:
	\begin{equation}
	\bJ_{R}(\by)= (\bI+\nabla^{2}R(\bprox_{R}(\by)))^{-1}= (\bI + \nabla^{2}R(\hat{\bu}))^{-1}= \left(\bI + \frac{1}{2\lambda_{2}}\bX_{E}\bX_{E}^{\top}\right)^{-1}
	\end{equation}
for $E = \{j:|\bX_{j}^{\top}\bu|>\lambda_{1}\}$.

\section{ALO for Elastic Net, Approximation with Proximal Formulation}
For the elastic net problem, the proximal mapping is known to be
	\begin{equation}
	\bprox_R\left(\bz\right)=\gamma\sign(\bz)\odot(|\bz|-\lambda\bm{1}_p)_+,\qquad\gamma=\frac{1}{1+(1-\alpha)\lambda}.
	\end{equation}
Let \(E\) be the active set, if \(z_i\in E\), then \[\frac{\partial}{\partial z_i}\gamma\sign(z_i)(|z_i|-\lambda)_+=\gamma.\] Plug in \(\bz=\hat{\bbeta}-\sum_{j=1}^{n}\dot{\ell}(\bx_j^\top\hat{\bbeta}\semicol y_j)\bx_j\), \refthm{Eqn.}{46} thus reduce to 
	\begin{equation}
	\bH=\gamma\bX_{\cdot,E}\left[\gamma\bX_{\cdot,E}^\top\bX_{\cdot,E}+\left(1-\gamma\right)\bI_{E,E}\right]^{-1}\bX_{\cdot,E}^\top.
	\end{equation}
Bringing back the intercept term is straightforward as well. Noted that \[\begin{bmatrix}
\hat{\bbeta}_0^{\setminus i} \\
\hat{\bm{\bbeta}}^{\setminus i}\end{bmatrix}=
\bprox_{R}\left(\bz\right),\qquad\bz=\begin{bmatrix}
\hat{\bbeta}_0^{\setminus i} \\
\hat{\bm{\bbeta}}^{\setminus i}\end{bmatrix}-
\sum_{j\neq i}\begin{bmatrix}
1 \\
\bm{x}_j\end{bmatrix}
\dot{\ell}\left(\hat{\bbeta}_0^{\setminus i}+\bm{x}_j^\top\hat{\bm{\bbeta}}^{\setminus i};y_j\right).\] Hence, from the first-order condition \(\sum_{j\neq i}\dot{\ell}\left(\hat{\bbeta}_0^{\setminus i}+\bm{x}_j^\top\hat{\bm{\bbeta}}^{\setminus i};y_j\right)=0\), we can derive that 
	\begin{equation}
	\bm{J}_{E,E}=
	\left[\bm{J}(\bm{u})\right]_{E,E}=
	\begin{bmatrix}
	1 & 0 & 0 & \dots & 0\\
	0 & (1-\alpha)\lambda & 0 & \dots & 0\\
	0 & 0 & (1-\alpha)\lambda & \dots & 0\\
	\vdots & \vdots & \vdots & \ddots & \vdots\\
	0 & 0 & 0 & 0 & (1-\alpha)\lambda\\
	\end{bmatrix}^{-1}.
	\end{equation}
The ALO formula is then immediate by \refthm{Thm.}{5.1}.

\section{ALO for LASSO, with Intercept through Generalized LASSO}
For the generalized LASSO:
	\begin{equation}
	\min\limits_{\bbeta}\frac{1}{2}\|\by-\bX\bbeta\|+\lambda\|\bD\bbeta\|_{1},
	\end{equation}
the dual problem can be derived as:
	\begin{equation}
	\min\limits_{\bu}\frac{1}{2}\|\by-\btheta\|_{2}^{2},\qquad\btheta\in \{\bX^{\top}\btheta = \bD^{\top}\bu, \|\bu\|_{\infty} \leq \lambda\}.
	\end{equation}
The dual problem could be written in a proximal approach, such that: \[\hat{\bu} = \bprox_{R}(\by),\qquad R(\bu) =\begin{dcases}
0 & \btheta \in \{\bX^{\top}\btheta = \bD^{\top}\bu, \|\bu\|_{\infty} \leq \lambda\},\\
\infty & \text{otherwise.}
\end{dcases}\] Denote $\bJ$ as the Jacobian of the proximal operator at the full data problem $\by$, then the ALO estimator could be obtained as: 
	\begin{equation}
	\by^{\setminus i} = \by_{i} - \frac{\hat{\bu}_{i}}{\bJ_{ii}}.
	\end{equation}
For the case of LASSO with an intercept, we could expand the $\bX$ with a column of ones in the first column, expand $\bbeta$ with another dimension and choose $\bD = [\bm{0}, \bI]$. Let \(E\coloneqq\{j:|\bX_{j}^{\top}\btheta| = \lambda \}\) denote the active set. The Jacobian is locally given as the projection onto the orthogonal complement of the span of $\bX_{E}$ and the vector of ones. Further denote $\tilde{\bX}_{E} = [\textbf{1}, \bX_{E}]$, then the Jacobian is given as $\bI - \tilde{\bX_{E}}(\tilde{\bX}_{E}^{\top}\tilde{\bX}_{E})\tilde{\bX}_{E}^{\top}$.

\section{ALO for Elastic Net, without Penalty on Intercept through Generalized LASSO}
Without penalty on intercept, the elastic net problem can be written as:
\begin{align*}
\begin{bmatrix}
\hat{\bbeta}_0 \\
\hat{\bbeta}
\end{bmatrix} &= \argmin\frac{1}{2} \left\|\by - \beta_0 - \bX\bbeta\right\|_2^2 + \lambda_1 \|\bbeta\|_1 + \lambda_2 \|\bbeta\|_2^2 \\
&= \argmin\frac{1}{2} 
\begin{bmatrix}
\beta_0 \\
\bbeta
\end{bmatrix}^\top \left(
\begin{bmatrix}
1 & \bX
\end{bmatrix}^\top
\begin{bmatrix}
1 & \bX
\end{bmatrix} + 
\lambda_2 \diag(0; \bm{1}_p)
\right)
\begin{bmatrix} 
\beta_0 \\
\bbeta
\end{bmatrix} - \by^\top
\begin{bmatrix}
1 & \bX
\end{bmatrix} 
\begin{bmatrix}
\beta_0 \\
\bbeta
\end{bmatrix}   + \lambda_1 \|\bbeta\|_1
\end{align*}
where we assume that the size of $\bX$ is $n \times p$. In the mean time, note the LASSO problem (also without penalty on intercept) is:
\begin{align*}
\begin{bmatrix}
\hat{\bbeta}_0 \\
\hat{\bbeta}
\end{bmatrix} &= \argmin\frac{1}{2} \left\|\by - \beta_0 - \bX\bbeta\right\|_2^2 + \lambda_1 \|\bbeta\|_1 \\
&= \argmin\frac{1}{2} 
\begin{bmatrix}
\beta_0 \\
\bbeta
\end{bmatrix}^\top 
\begin{bmatrix}
1 & X
\end{bmatrix}^\top
\begin{bmatrix}
1 & X
\end{bmatrix} 
\begin{bmatrix} 
\beta_0 \\
\bbeta
\end{bmatrix} - \by^\top
\begin{bmatrix}
1 & X
\end{bmatrix} 
\begin{bmatrix}
\beta_0 \\
\bbeta
\end{bmatrix} +  \lambda_1 \|\bbeta\|_1
\end{align*}
Thus we can add some ``observations'' to the data and let
$$\by^\ast = \begin{bmatrix}
\by \\
\bm0_p
\end{bmatrix},\qquad \bX^\ast =
\begin{bmatrix}
\bX \\
\sqrt{\lambda_2} \bm{I}_p
\end{bmatrix}, $$
then the elastic net becomes
	\begin{equation}
		\begin{aligned}
		\begin{bmatrix}
		\hat{\bbeta}_0 \\
		\hat{\bbeta}
		\end{bmatrix} &= \argmin\frac{1}{2} \left\|\by^\ast - \bbeta_0
		\begin{bmatrix}
		\bm{1}_n \\
		\bm{0}_p
		\end{bmatrix} - \bX^\ast\bbeta\right\|_2^2 + \lambda_1 \|\bbeta\|_1  \\
		&= \argmin\frac{1}{2} \left\|
		\begin{bmatrix}
		\by \\
		\bm0_p
		\end{bmatrix}
		- 
		\begin{bmatrix}
		\bm{1}_n & \bX \\
		\bm{0}_p &\sqrt{\lambda_2} \bm{I}_p
		\end{bmatrix}  
		\begin{bmatrix}
		\bbeta_0 \\
		\bbeta
		\end{bmatrix} 
		\right \|_2^2 + \lambda_1 \|\bbeta\|_1,
		\end{aligned}
	\end{equation}
which is a special case of the general LASSO.

\section{Generalized Loss function}
\subsection{ALO for Logistic Regression, Approximation in the Primal Domain}
For binomial logistic regression, the primal problem is: \[\min_{\bbeta}\sum_{j=1}^{n}\left[\ln\left(1+e^{\bx_j^\top\bbeta}\right)-y_j\bx_j^\top\bbeta\right]+\lambda\left(\alpha\|\bbeta\|_1+\frac{1-\alpha}{2}\|\bbeta\|_2^2\right).\] Let \(A\) be the active set, we have \[\dot{\ell}(\bx_j^\top\bbeta\semicol y_j)=\frac{e^{\bx_j^\top\bbeta}}{1+e^{\bx_j^\top\bbeta}}-y_j,\qquad\ddot{\ell}(\bx_j^\top\bbeta\semicol y_j)=\frac{e^{\bx_j^\top\bbeta}}{\left(1+e^{\bx_j^\top\bbeta}\right)^2},\qquad\nabla^2 R(\hat{\bbeta}_A)=(1-\alpha)\lambda\bI_{A,A}.\] Therefore, the ALO formula is \[\bx_i^\top\tilde{\bbeta}^{\setminus j}\approx\bx_i^\top\hat{\bbeta}+\frac{\bH_{ii}\left(1+e^{\bx_j^\top\hat{\bbeta}}\right)\left[e^{\bx_j^\top\hat{\bbeta}}-y_j\left(1+e^{\bx_j^\top\hat{\bbeta}}\right)\right]}{\left(1+e^{\bx_j^\top\hat{\bbeta}}\right)^2-\bH_{ii}e^{\bx_j^\top\hat{\bbeta}}},\] where \[\bH=\bX_{\cdot,A}\left[\bX_{\cdot,A}^\top\diag\left(\frac{e^{\bx_j^\top\hat{\bbeta}}}{1+2e^{\bx_j^\top\hat{\bbeta}}+e^{2\bx_j^\top\hat{\bbeta}}}\right)\bX_{\cdot,A}+(1-\alpha)\lambda\bI_{A,A}\right]^{-1}\bX_{\cdot,A}^\top.\] Adding in intercept terms is then straightforward.

\subsection{ALO for Logistic Regression, Approximation in the Dual Domain}
\subsubsection{ALO for Logistic Regression with Lasso penalty}
First, let's rewrite the optimization problem with the loss functions separated for each observation, therefore the loss function goes: \[-\sum_{i=1}^n\left[y_{i}\bx_{i}^{\top}\bbeta+\ln\left(1+e^{\bx_{i}^{\top}\bbeta}\right)\right]+\lambda_{1}\|\bbeta\|_{1},\]
where the individual loss function is $\ell(x_{i}^{\top}\bbeta ;y_{i}) = y_{i}x_{i}^{\top}\bbeta+\ln(1+e^{\bx_{i}^{\top}\bbeta})$ and the regularizer is $R(\bbeta) = \lambda_{1}\|\bbeta\|_{1}$, from which we could derive the dual optimal and the conjugate functions: \[\hat{\btheta} = y - \frac{e^{\bX\hat{\bbeta}}}{1+e^{\bX\hat{\bbeta}}},\qquad \ell^{*}(-\theta_{i};y_{i})=(y_{i}-\theta_{i})\ln\frac{y_{i}-\theta_{i}}{1-(y_{i}-\theta_{i})}-\ln\frac{1}{1-(y_{i}-\theta_{i})},\qquad R^{*}(\bbeta) =\begin{dcases}
0 & \|\bbeta\|_{\infty} \leq \lambda_{1},\\
\infty & \text{otherwise}.\\
\end{dcases}\]

From the results of the conjugate functions above, we could also obtain the derivatives of the loss functions and the Jacobian of the regularizer: \[\dot{\ell}^{*}(-\theta_{i};y_{i}) = \ln \frac{y_{i}-\theta_{i}}{1-(y_{i}-\theta_{i})},\qquad \ddot{\ell}^{*}(-\theta_{i};y_{i}) = \frac{1}{(y_{i}-\theta_{i})[1-(y_{i}-\theta_{i})]}\]

Recall \refthm{Eqn.}{20} from the main paper, the quadratic surrogate of the dual problem is \[\min\limits_{\bu} \frac{1}{2}\sum_{i=1}^n\left(u_{i}-\frac{\hat{\theta}_{i}\ddot{\ell}^{*}(-\hat{\theta}_{i};y_{i})+\hat{y}_{i}}{\sqrt{\ddot{\ell}^{*}(-\hat{\theta}_{i};y_{i})}}\right)^{2}+R^{*}(X^{\top}Ku),\] where $\bK=\diag\sqrt{\ddot{\ell}^{*}(-\hat{\theta}_{i};y_{i})}$. Therefore the Jacobian at $\by_{u} =\hat{\theta}_{i}\ddot{\ell}^{*}(-\hat{\theta}_{i};y_{i})+\hat{\theta}_{i}/\sqrt{\ddot{\ell}^{*}(-\hat{\theta}_{i};y_{i})}$ could locally be treated as the projection onto the orthogonal complement of the polyhedron $\{\|\bX^{\top}K\bu\|_{\infty} \leq \lambda_{1}\}$, thus $\bJ = \bI - \bX_{u,E}(\bX_{u,E}^{\top}\bX_{u,E})^{-1}\bX_{u,E}$, where $\bX_{u,E}$ are the columns of $\bX_{u}=\bX^{\top}\bK$, such that the columns in the set $E = \{i:|\bX_{i}^{\top}\btheta| = \lambda_{1}\}$ are selected. Take everything to \refthm{Eqn.}{22}, $y^{/i} = \bK_{ii}(y_{u,i}-\bK_{ii}\hat{\theta}_{i}/\bJ_{ii})$, we could obtain the ALO for the $i$-th observation.

\subsubsection{ALO for Logistic Regression with Elastic Net Penalty}
The optimization problem for logistic regression with elastic net penalty is: \[-\sum_{i=1}^n\left[y_{i}x_{i}^{\top}\bbeta+\ln\left(1+e^{\bx_{i}^{\top}\bbeta}\right)\right]+\lambda_{1}\|\bbeta\|_{1}+\lambda_{2}\|\bbeta\|_{2}^{2}.\]
The optimization problem is the same except the regularizer is changed, therefore the only thing different is the conjugate function of the regularizer, $R^{*}$ and the corresponding Jacobian, here $R(\bbeta) = \lambda_{1}\|\bbeta\|_{1}+\lambda_{2}\|\bbeta\|_{2}^{2}$: \[R^{*}(\bbeta) = \sum\limits_{|u_{i}| > \lambda_{1}} \frac{(\lambda_{1}-|u_{i}|)^{2}}{4\lambda_{2}}.\] The corresponding Jacobian is $\bJ = (\bI +\bX_{u,E}\bX_{u,E}^{\top}/2\lambda_{2})$, where $\bX_{u,E}$ are the columns of $\bX_{u}=\bX^{\top}\bK$, such that the columns in the set $E = \{i:|\bX_{i}^{\top}\btheta| = \lambda_{1}\}$ are selected. Take everything to \refthm{Eqn.}{22}, $y^{/i} = \bK_{ii}(y_{u,i}-\bK_{ii}\hat{\theta}_{i}/\bJ_{ii})$, we could obtain the ALO for the $i$-th observation.

\subsection{ALO for Poisson Regression, Approximation in the Primal Domain}
For Poisson regression, the primal problem is: \[\min_{\bbeta}\sum_{j=1}^{n}\left(e^{\bx_j^\top\bbeta}-y_j\bx_j^\top\bbeta\right)+\lambda\left(\alpha\|\bbeta\|_1+\frac{1-\alpha}{2}\|\bbeta\|_2^2\right).\] Let \(A\) be the active set, we have \[\dot{\ell}(\bx_j^\top\bbeta\semicol y_j)=e^{\bx_j^\top\bbeta}-y_j,\qquad\ddot{\ell}(\bx_j^\top\bbeta\semicol y_j)=e^{\bx_j^\top\bbeta},\qquad\nabla^2 R(\hat{\bbeta}_A)=(1-\alpha)\lambda\bI_{A,A}.\] Therefore, the ALO formula is \[\bx_i^\top\tilde{\bbeta}^{\setminus j}\approx\bx_i^\top\hat{\bbeta}+\frac{\bH_{ii}\left(e^{\bx_j^\top\hat{\bbeta}}-y_j\right)}{1-\bH_{ii}e^{\bx_j^\top\hat{\bbeta}}},\] where \[\bH=\bX_{\cdot,A}\left[\bX_{\cdot,A}^\top\diag\left(e^{\bx_j^\top\hat{\bbeta}}\right)\bX_{\cdot,A}+(1-\alpha)\lambda\bI_{A,A}\right]^{-1}\bX_{\cdot,A}^\top.\]

\subsection{ALO for Poisson Regression, Approximation in the Dual Domain}
\subsubsection{ALO for Poisson Regression with Lasso penalty}
The optimization function for Poisson regression with lasso penalty is: \[\sum_{i=1}^n\left[-y_{i}\bx_{i}^{\top}\bbeta+e^{\bx_{i}^{\top}\bbeta}+\ln(y_{i}!)\right] + \lambda_{1}\|\bbeta\|_{1}\] The regularizer is the same as the logistic regression with the lasso penalty case, thus the Jacobian will also be the same, therefore we only have to focus on the loss function $\ell(x_{i}^{\top}\bbeta ; y_{i}) = -y_{i}x_{i}^{\top}\bbeta+e^{x_{i}^{\top}\bbeta}+\ln(y_{i}!)$. The optimal solution for the dual problem $\hat{\theta} = y - e^{\bX\hat{\bbeta}}$ and the conjugate of the loss function is $\ell^{*}(-\theta_{i};y_{i}) = (y_{i}-\theta_{i})\ln(y_{i}-\theta_{i})-(y_{i}-\theta_{i})$, the corresponding derivatives are therefore: \[\dot{\ell}^{*}(-\theta_{i};y_{i}) = \ln(y_{i}-\theta_{i}),\qquad\ddot{\ell}^{*}(-\theta_{i};y_{i}) = \frac{1}{y_{i}-\theta_{i}}.\] By plugging everything into \refthm{Eqn.}{22}, we obtain the ALO for Poisson regression with the lasso penalty.

\subsubsection{ALO for Poisson Regression with Elastic Net Penalty}
The loss function for Poisson regression with elastic net penalty is the same as that of Poisson regression with the lasso penalty and the regularizer of it is the same as that of logistic regression with elastic net penalty. Thus by plugging everything into \refthm{Eqn.}{22}, we could obtain the ALO for Poisson regression with elastic net penalty.

\section{Usage of ALO formulae with \texttt{glmnet} package}
The \verb|glmnet| package scales the elastic net loss function by a factor of \(1/n\). Furthermore, for linear problems \verb|glmnet| implicitly ``standardizes \(y\) to have unit variance before computing its \(\lambda\) sequence (and then unstandardizes the resulting coefficients)'' (\emph{cf.} [\href{https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html}{Glmnet Vignette}]). So to get comparable results, it is necessary to rescale \(\by\) by the MLE \(\hat{\sigma}_y\) before fitting the model. More precisely, \verb|glmnet| is in fact optimizing the following problem: 
	\begin{equation}
	\min_{\bbeta}\frac{1}{2n}\sum_{j=1}^{n}\left(\frac{\bx_j^\top\bbeta}{\hat{\sigma}_y}-\frac{y_j}{\hat{\sigma}_y}\right)^2+\frac{\lambda}{\hat{\sigma}_y}\alpha\|\bbeta\|_1+\frac{\lambda}{\hat{\sigma}^2_y}\frac{1-\alpha}{2}\|\bbeta\|_2^2.
	\end{equation}
We thus have \[\dot{\ell}(\bx_j^\top\bbeta\semicol y_j)=\frac{\bx_j^\top\bbeta}{n\hat{\sigma}_y}-\frac{y_j}{n\hat{\sigma}_y},\qquad\ddot{\ell}(\bx_j^\top\bbeta\semicol y_j)=\frac{1}{n\hat{\sigma}_y},\qquad\nabla^2 R(\hat{\bbeta}_A)=\frac{(1-\alpha)\lambda}{\hat{\sigma}_y^2}\bI_{A,A}.\] Hence, for the linear elastic net problem, the primal ALO is: \[\tilde{y}_j^{\setminus i}=\hat{y}_j+\frac{\bH_{ii}(\hat{y}_j-y_j)}{n\hat{\sigma}_y-\bH_{ii}},\qquad\bH=\bX_{\cdot,A}\left[\frac{1}{n\hat{\sigma}_y}\bX_{\cdot,A}^\top\bX_{\cdot,A}+\frac{\left(1-\alpha\right)\lambda}{\hat{\sigma}_y^2}\bI_{A,A}\right]^{-1}\bX_{\cdot,A}^\top.\]
We get further complication when option \verb|standardization = T| is given, in which case \verb|glmnet| first standardize the data \(\bX\) using \(\hat{\sigma}_{\bX}\) (assuming \(\bX\) is standardized):
	\begin{itemize}
		\item If \verb|intercept = F|, compute \(\bX^\ast=\diag[\hat{\sigma}_y\hat{\sigma}_{\bX}]^{-1}\bX\).
		\item If \verb|intercept = T|, compute \(\bX^\ast=\diag[\hat{\sigma}_y\hat{\sigma}_{\bX}]^{-1}(\bX-\bar{\bX}\bm{1}\bm{1}^\top)\).
	\end{itemize} Afterwards, the the coefficients are returned de-standardized:
	\begin{itemize}
		\item Let \((\beta_0, \bbeta)\) be the original intercept and coefficients, compute \(\bbeta^\ast=\hat{\sigma}_y\diag[\hat{\sigma}_{\bX}]^{-1}\bbeta\) then \(\beta_0^\ast=\beta_0-\bar{\bX}\bbeta^\ast\). 
		\item Return pair \((\beta_0^\ast, \bbeta^\ast)\).
	\end{itemize}
For logistics and Poisson regression the standardization procedure is basically the same, except \verb|glmnet| no longer standardize by \(\hat{\sigma}_y\), which make sense since \(\by\) is now either categorical or count data.

\section{Benchmark}
\begin{table}[H]
	\centering
	\begin{tabular}{ccc|cc|c|c}
		\hline\hline
		\(n\) & \(p\) & \(k\) & Average ALO & Average 5-fold CV & Relative & \(n\) full fit \\\hline
		300 & 100 & 60 & 0.016 & 0.053 & 3.313 & 1.2\\
		500 & 800 & 500 & 0.251 & 0.533 & 2.124 & 36.5\\
		1000 & 1200 & 800 & 0.489 & 1.200 & 2.454 & 211.0\\
		2500 & 2000 & 1200 & 2.267 & 3.623 & 1.598 & 1577.5\\
		5000 & 2500 & 2000 & 5.017 & 8.097 & 1.614 & 7740.0\\
		10000 & 10000 & 2500 & 27.236 & 36.520 & 1.341 & 62530.0\\\hline
	\end{tabular} 
	\caption{Average elapsed time (in seconds) comparison, 10 runs, \(\alpha=0.5\).}
\end{table}
\end{document}